# -*- coding: utf-8 -*-
"""prefgooglemerge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_o7mRESOzbiekvqsR8IpcchWj_O5Cu9n
"""

import pandas as pd
import matplotlib.pyplot as plt


""" Some common errors, like the presence of the TP of Minas Gerais, are filtered """
def clean_url(url):
  if type(url) != str: return ''
  cleaned_url = url.replace("https://www.","").replace("http://www.","").replace("http://","").replace("https://","")
  if cleaned_url.endswith('/'): cleaned_url = cleaned_url[:-1]
  return cleaned_url

def remove_invalid_urls(links_to_filter, city_link1, city_link2):
  ans = []
  for x in links_to_filter: 
   if type(x) == str: 
    if ('transparencia.gov.br/localidades' not in x and 
        'www.transparencia.mg.gov.br' not in x and
        'www.portaltransparencia.gov.br' not in x and 
        clean_url(x) not in clean_url(city_link1) and
        clean_url(x) not in clean_url(city_link2)): 
      ans.append(x)
  return ans


""" Recieves a string containing all URLS and returns a list """
def get_url_array_from_pref(index, column):
  delimiter = '; '
  return remove_invalid_urls((df_pref.iloc[index, column]).split(delimiter),  df_pref.iloc[index, 2], df_pref.iloc[index, 3])

"""**Iterates through google search links searching for a intersection on the urls extracted in the city website**
For each of the 6 links taken from the google search, we check if it was found on its respective city hall website, 
otherwise we check if theres a more likely link within the city hall website. 
In the last case, the first valid link of the google search is chosen.
"""
def get_intersection(google_search, urls_pref):
  for i in google_search: 
    for string in urls_pref: 
      if clean_url(i) in string: return i
  return ''

def find_correct_url(df_google, df_pref):
  urls = []
  for index, row in df_google.iterrows():
    google_urls = remove_invalid_urls(row.iloc[2:8], df_pref.iloc[index, 2], df_pref.iloc[index, 3])
    if type(df_pref.iloc[index, 4]) == str: #urls were found in cities website
      all_urls_pref = get_url_array_from_pref(index, 4)
      url = get_intersection(google_urls, all_urls_pref)
      if url != '': # theres an intersection
        urls.append(url)
      elif type(df_pref.iloc[index, 5]) == str:
        most_likely_urls_pref = get_url_array_from_pref(index, 5)
        urls.append(most_likely_urls_pref[0])
      elif google_urls: urls.append(google_urls[0])
      else: urls.append('')
    elif google_urls: urls.append(google_urls[0])
    else: urls.append('')
  
  return urls


df_google = pd.read_csv('retirados_google.csv')
df_pref = pd.read_csv('retirados_prefeitura.csv')
urls = find_correct_url(df_google, df_pref)

""" Save correct urls in new DataFrame"""
new_df = pd.DataFrame({'Municipios': df_pref.iloc[:, 1], 'Site Prefeitura': df_pref.iloc[:, 2], 'Site Camara': df_pref.iloc[:, 3], 'Porta da TransparÃªncia': urls})
new_df.to_csv('portais_transparencia_filtrados.csv')
